{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776ac962",
   "metadata": {},
   "source": [
    "## Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56f3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images exists: True\n",
      "Number of files in processed_images: 99\n",
      "['AN_10_G3_back_0.jpg', 'AN_10_G3_neck_1.jpg', 'AN_11_G4_back_0.jpg', 'AN_11_G4_neck_1.jpg', 'AN_12_G2_back_0.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Processed images exists:\", os.path.exists(\"../data/processed_images\"))\n",
    "print(\"Number of files in processed_images:\",\n",
    "      len(os.listdir(\"../data/processed_images\")))\n",
    "print(os.listdir(\"../data/processed_images\")[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e8d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../data/cropped_images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50567ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Haarcascade face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \n",
    "                                     \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "INPUT_DIR = \"../data/processed_images\"\n",
    "OUTPUT_DIR = \"../data/cropped_images\"\n",
    "\n",
    "def crop_neck_region(image_path, save_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return False\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        # fallback: bottom-center crop\n",
    "        y1 = int(h * 0.45)\n",
    "        y2 = h\n",
    "        x1 = int(w * 0.15)\n",
    "        x2 = int(w * 0.85)\n",
    "        neck_crop = img[y1:y2, x1:x2]\n",
    "    else:\n",
    "        (x, y, w_box, h_box) = faces[0]\n",
    "\n",
    "        chin_y = y + int(h_box * 0.9)\n",
    "\n",
    "        y1 = max(chin_y, 0)\n",
    "        y2 = min(chin_y + int(h * 0.35), h)\n",
    "\n",
    "        x1 = max(x - int(w_box * 0.3), 0)\n",
    "        x2 = min(x + w_box + int(w_box * 0.3), w)\n",
    "\n",
    "        neck_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "    if neck_crop.size == 0:\n",
    "        return False\n",
    "\n",
    "    cv2.imwrite(save_path, neck_crop)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202b01a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:02<00:00, 40.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping done ‚úÖ\n",
      "Successful: 99\n",
      "Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "image_paths = glob.glob(os.path.join(INPUT_DIR, \"*\"))\n",
    "\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Cropping images\"):\n",
    "    filename = os.path.basename(img_path)\n",
    "    save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    if crop_neck_region(img_path, save_path):\n",
    "        success += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(f\"Cropping done ‚úÖ\")\n",
    "print(f\"Successful: {success}\")\n",
    "print(f\"Failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2e1c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropped images: 99\n",
      "['AN_10_G3_back_0.jpg', 'AN_10_G3_neck_1.jpg', 'AN_11_G4_back_0.jpg', 'AN_11_G4_neck_1.jpg', 'AN_12_G2_back_0.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cropped images:\",\n",
    "      len(os.listdir(\"../data/cropped_images\")))\n",
    "print(os.listdir(\"../data/cropped_images\")[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c83e6",
   "metadata": {},
   "source": [
    "## Canonical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b74cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Phase 0 complete\n",
      "Total images: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>grade</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/cropped_images/AN_1_G4_back_0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/cropped_images/AN_1_G4_neck_1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/cropped_images/AN_1_G4_neck_2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/cropped_images/AN_1_G4_neck_3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/cropped_images/AN_2_G3_back_0.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_path  patient_id  grade  view\n",
       "0  data/cropped_images/AN_1_G4_back_0.jpg           1      4  back\n",
       "1  data/cropped_images/AN_1_G4_neck_1.jpg           1      4  neck\n",
       "2  data/cropped_images/AN_1_G4_neck_2.jpg           1      4  neck\n",
       "3  data/cropped_images/AN_1_G4_neck_3.jpg           1      4  neck\n",
       "4  data/cropped_images/AN_2_G3_back_0.jpg           2      3  back"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "CROPPED_DIR = \"../data/cropped_images\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for filename in tqdm(os.listdir(CROPPED_DIR), desc=\"Building dataset\"):\n",
    "    if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    # Expected filename pattern:\n",
    "    # AN_10_G3_back_0.jpg\n",
    "    parts = filename.split(\"_\")\n",
    "\n",
    "    try:\n",
    "        patient_id = int(parts[1])\n",
    "        grade = int(parts[2].replace(\"G\", \"\"))\n",
    "        view = parts[3].lower() if len(parts) > 3 else \"unknown\"\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping malformed filename: {filename} | {e}\")\n",
    "        continue\n",
    "\n",
    "    rows.append({\n",
    "        \"image_path\": f\"data/cropped_images/{filename}\",\n",
    "        \"patient_id\": patient_id,\n",
    "        \"grade\": grade,\n",
    "        \"view\": view\n",
    "    })\n",
    "\n",
    "assert len(rows) > 0, \"‚ùå No images found ‚Äî something is wrong\"\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.sort_values(by=[\"patient_id\", \"view\"]).reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"../data/dataset.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Phase 0 complete\")\n",
    "print(\"Total images:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36c6b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images per grade:\n",
      "grade\n",
      "4    39\n",
      "3    36\n",
      "2    12\n",
      "1    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique patients: 35\n",
      "\n",
      "Images per patient (first 10):\n",
      "patient_id\n",
      "1     4\n",
      "2     2\n",
      "3     2\n",
      "4     2\n",
      "5     1\n",
      "6     2\n",
      "7     2\n",
      "8     2\n",
      "9     2\n",
      "10    2\n",
      "dtype: int64\n",
      "\n",
      "Views:\n",
      "view\n",
      "neck    64\n",
      "back    35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Images per grade:\")\n",
    "print(df[\"grade\"].value_counts())\n",
    "\n",
    "print(\"\\nUnique patients:\", df[\"patient_id\"].nunique())\n",
    "\n",
    "print(\"\\nImages per patient (first 10):\")\n",
    "print(df.groupby(\"patient_id\").size().head(10))\n",
    "\n",
    "print(\"\\nViews:\")\n",
    "print(df[\"view\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff67f2",
   "metadata": {},
   "source": [
    "## Patient-Wise cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df24edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Phase 1 complete\n",
      "Saved 5 patient-wise folds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "N_FOLDS = 5\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "folds = {}\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(\n",
    "        gkf.split(df, groups=df[\"patient_id\"])):\n",
    "\n",
    "    folds[f\"fold_{fold}\"] = {\n",
    "        \"train_idx\": train_idx.tolist(),\n",
    "        \"test_idx\": test_idx.tolist()\n",
    "    }\n",
    "\n",
    "# Save splits\n",
    "os.makedirs(\"../splits\", exist_ok=True)\n",
    "with open(\"../splits/cv_folds.json\", \"w\") as f:\n",
    "    json.dump(folds, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Phase 1 complete\")\n",
    "print(f\"Saved {N_FOLDS} patient-wise folds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222f32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_0 overlap: set()\n",
      "fold_1 overlap: set()\n",
      "fold_2 overlap: set()\n",
      "fold_3 overlap: set()\n",
      "fold_4 overlap: set()\n"
     ]
    }
   ],
   "source": [
    "for fold_name, split in folds.items():\n",
    "    train_patients = set(df.iloc[split[\"train_idx\"]][\"patient_id\"])\n",
    "    test_patients  = set(df.iloc[split[\"test_idx\"]][\"patient_id\"])\n",
    "\n",
    "    overlap = train_patients & test_patients\n",
    "    print(f\"{fold_name} overlap:\", overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf9156",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82984c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../models/effnet_b0\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f894931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9806801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        label = int(row[\"grade\"]) - 1  # 1‚Äì4 ‚Üí 0‚Äì3\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5000d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.3, 0.3, 0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1836c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        correct += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            gts.extend(labels.numpy())\n",
    "\n",
    "    return accuracy_score(gts, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8eaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "with open(\"../splits/cv_folds.json\") as f:\n",
    "    folds = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46aa4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed image paths in dataset.csv\n",
      "0    ../data/cropped_images/AN_1_G4_back_0.jpg\n",
      "1    ../data/cropped_images/AN_1_G4_neck_1.jpg\n",
      "2    ../data/cropped_images/AN_1_G4_neck_2.jpg\n",
      "3    ../data/cropped_images/AN_1_G4_neck_3.jpg\n",
      "4    ../data/cropped_images/AN_2_G3_back_0.jpg\n",
      "Name: image_path, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# Fix image paths (add ../)\n",
    "df[\"image_path\"] = df[\"image_path\"].apply(\n",
    "    lambda x: \"../\" + x if not x.startswith(\"../\") else x\n",
    ")\n",
    "\n",
    "df.to_csv(\"../data/dataset.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Fixed image paths in dataset.csv\")\n",
    "print(df[\"image_path\"].head()) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad3ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD_0 =====\n",
      "fold_0 accuracy: 0.2500\n",
      "\n",
      "===== FOLD_1 =====\n",
      "fold_1 accuracy: 0.4000\n",
      "\n",
      "===== FOLD_2 =====\n",
      "fold_2 accuracy: 0.2500\n",
      "\n",
      "===== FOLD_3 =====\n",
      "fold_3 accuracy: 0.3500\n",
      "\n",
      "===== FOLD_4 =====\n",
      "fold_4 accuracy: 0.5789\n"
     ]
    }
   ],
   "source": [
    "fold_accuracies = []\n",
    "\n",
    "for fold_name, split in folds.items():\n",
    "    print(f\"\\n===== {fold_name.upper()} =====\")\n",
    "\n",
    "    train_df = df.iloc[split[\"train_idx\"]]\n",
    "    test_df  = df.iloc[split[\"test_idx\"]]\n",
    "\n",
    "    train_ds = ANDataset(train_df, train_tfms)\n",
    "    test_ds  = ANDataset(test_df, test_tfms)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
    "    model.to(device)\n",
    "\n",
    "    # Class weights (per fold!)\n",
    "    y = train_df[\"grade\"].values - 1\n",
    "    weights = compute_class_weight(\"balanced\", classes=np.array([0,1,2,3]), y=y)\n",
    "    weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(12):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "    # Evaluate\n",
    "    test_acc = eval_model(model, test_loader, device)\n",
    "    fold_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"{fold_name} accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"../models/effnet_b0/{fold_name}.pth\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfee972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL CV RESULT =====\n",
      "Fold accuracies: [0.25, 0.4, 0.25, 0.35, 0.5789473684210527]\n",
      "Mean accuracy: 0.3658\n",
      "Std deviation: 0.1214\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== FINAL CV RESULT =====\")\n",
    "print(\"Fold accuracies:\", fold_accuracies)\n",
    "print(f\"Mean accuracy: {np.mean(fold_accuracies):.4f}\")\n",
    "print(f\"Std deviation: {np.std(fold_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08ba40",
   "metadata": {},
   "source": [
    "We evaluated the baseline EfficientNet-B0 model using 5-fold patient-wise cross-validation, ensuring that images from the same patient never appear in both training and testing sets.\n",
    "\n",
    "Interpretation\n",
    "\n",
    "The variability across folds is expected due to:\n",
    "\n",
    "-Small dataset (35 patients)\n",
    "\n",
    "-Uneven grade distribution\n",
    "\n",
    "-High inter-patient variability in skin tone, lighting, and image quality\n",
    "\n",
    "The higher accuracy in Fold 4 indicates that the model can perform well when the test patients are visually distinct.\n",
    "\n",
    "This mean accuracy represents a true, unbiased baseline, unlike a single train/test split which can be misleading on small medical datasets.\n",
    "\n",
    "This result establishes a reliable reference point against which all future models will be compared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b52738",
   "metadata": {},
   "source": [
    "# Model 2: EfficientNet-B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e54ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../models/effnet_b2\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb86c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0ceae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((260, 260)),   # B2 default ~260\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.3, 0.3, 0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((260, 260)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afb253c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD_0 (EffNet-B2) =====\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to C:\\Users\\Pranathi/.cache\\torch\\hub\\checkpoints\\efficientnet_b2_rwightman-c35c1473.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35.2M/35.2M [00:15<00:00, 2.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_0 accuracy: 0.2500\n",
      "\n",
      "===== FOLD_1 (EffNet-B2) =====\n",
      "fold_1 accuracy: 0.4500\n",
      "\n",
      "===== FOLD_2 (EffNet-B2) =====\n",
      "fold_2 accuracy: 0.2500\n",
      "\n",
      "===== FOLD_3 (EffNet-B2) =====\n",
      "fold_3 accuracy: 0.3500\n",
      "\n",
      "===== FOLD_4 (EffNet-B2) =====\n",
      "fold_4 accuracy: 0.5263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "fold_accuracies_b2 = []\n",
    "\n",
    "for fold_name, split in folds.items():\n",
    "    print(f\"\\n===== {fold_name.upper()} (EffNet-B2) =====\")\n",
    "\n",
    "    train_df = df.iloc[split[\"train_idx\"]]\n",
    "    test_df  = df.iloc[split[\"test_idx\"]]\n",
    "\n",
    "    train_ds = ANDataset(train_df, train_tfms)\n",
    "    test_ds  = ANDataset(test_df, test_tfms)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=6, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=6, shuffle=False)\n",
    "\n",
    "    # üî• Model: EfficientNet-B2\n",
    "    model = efficientnet_b2(weights=\"IMAGENET1K_V1\")\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
    "    model.to(device)\n",
    "\n",
    "    # Class weights (per fold)\n",
    "    y = train_df[\"grade\"].values - 1\n",
    "    weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.array([0,1,2,3]),\n",
    "        y=y\n",
    "    )\n",
    "    weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(14):  # slightly more than B0\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "    # Evaluate\n",
    "    test_acc = eval_model(model, test_loader, device)\n",
    "    fold_accuracies_b2.append(test_acc)\n",
    "\n",
    "    print(f\"{fold_name} accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"../models/effnet_b2/{fold_name}.pth\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1af16c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EFFICIENTNET-B2 CV RESULT =====\n",
      "Fold accuracies: [0.25, 0.45, 0.25, 0.35, 0.5263157894736842]\n",
      "Mean accuracy: 0.3653\n",
      "Std deviation: 0.1095\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== EFFICIENTNET-B2 CV RESULT =====\")\n",
    "print(\"Fold accuracies:\", fold_accuracies_b2)\n",
    "print(f\"Mean accuracy: {np.mean(fold_accuracies_b2):.4f}\")\n",
    "print(f\"Std deviation: {np.std(fold_accuracies_b2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae1cb9",
   "metadata": {},
   "source": [
    "# Model-3 Ordinal Regression with EfficientNet-B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "591aa17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANDatasetReg(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        y = float(row[\"grade\"])  # keep 1‚Äì4\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28c092c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f44a0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8ded98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_reg(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, y in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs).squeeze(1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_reg(model, loader, device):\n",
    "    model.eval()\n",
    "    preds_all, y_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, y in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            preds = model(imgs).squeeze(1)\n",
    "\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            y_all.extend(y.numpy())\n",
    "\n",
    "    preds_all = np.array(preds_all)\n",
    "    y_all = np.array(y_all)\n",
    "\n",
    "    mae = np.mean(np.abs(preds_all - y_all))\n",
    "\n",
    "    # Off-by-1 accuracy (round to nearest grade)\n",
    "    rounded = np.clip(np.round(preds_all), 1, 4)\n",
    "    off_by_1 = np.mean(np.abs(rounded - y_all) <= 1)\n",
    "\n",
    "    return mae, off_by_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98ea68c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD_0 (REGRESSION) =====\n",
      "fold_0 MAE: 0.663 | Off-by-1 Acc: 0.900\n",
      "\n",
      "===== FOLD_1 (REGRESSION) =====\n",
      "fold_1 MAE: 0.971 | Off-by-1 Acc: 0.800\n",
      "\n",
      "===== FOLD_2 (REGRESSION) =====\n",
      "fold_2 MAE: 0.985 | Off-by-1 Acc: 0.750\n",
      "\n",
      "===== FOLD_3 (REGRESSION) =====\n",
      "fold_3 MAE: 1.082 | Off-by-1 Acc: 0.750\n",
      "\n",
      "===== FOLD_4 (REGRESSION) =====\n",
      "fold_4 MAE: 1.125 | Off-by-1 Acc: 0.684\n"
     ]
    }
   ],
   "source": [
    "maes = []\n",
    "off1s = []\n",
    "\n",
    "for fold_name, split in folds.items():\n",
    "    print(f\"\\n===== {fold_name.upper()} (REGRESSION) =====\")\n",
    "\n",
    "    train_df = df.iloc[split[\"train_idx\"]]\n",
    "    test_df  = df.iloc[split[\"test_idx\"]]\n",
    "\n",
    "    train_ds = ANDatasetReg(train_df, train_tfms)\n",
    "    test_ds  = ANDatasetReg(test_df, test_tfms)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(12):\n",
    "        train_loss = train_one_epoch_reg(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "    mae, off1 = eval_reg(model, test_loader, device)\n",
    "    maes.append(mae)\n",
    "    off1s.append(off1)\n",
    "\n",
    "    print(f\"{fold_name} MAE: {mae:.3f} | Off-by-1 Acc: {off1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b6f288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== REGRESSION CV RESULT =====\n",
      "MAE per fold: [np.float32(0.6629048), np.float32(0.9706052), np.float32(0.9848345), np.float32(1.0822408), np.float32(1.1249658)]\n",
      "Mean MAE: 0.965 ¬± 0.162\n",
      "Off-by-1 Acc per fold: [np.float64(0.9), np.float64(0.8), np.float64(0.75), np.float64(0.75), np.float64(0.6842105263157895)]\n",
      "Mean Off-by-1 Acc: 0.777\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== REGRESSION CV RESULT =====\")\n",
    "print(\"MAE per fold:\", maes)\n",
    "print(f\"Mean MAE: {np.mean(maes):.3f} ¬± {np.std(maes):.3f}\")\n",
    "\n",
    "print(\"Off-by-1 Acc per fold:\", off1s)\n",
    "print(f\"Mean Off-by-1 Acc: {np.mean(off1s):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03fc01",
   "metadata": {},
   "source": [
    "The regression model demonstrates that although exact grade prediction remains challenging, the model consistently predicts AN severity within one grade of the clinician-assigned label in a majority of cases.\n",
    "\n",
    "This is far more realistic than claiming exact accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd4710",
   "metadata": {},
   "source": [
    "# Patient-level multi view fusion on regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fec2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reg_patient_level(model, loader, df_subset, device):\n",
    "    \"\"\"\n",
    "    df_subset: dataframe corresponding exactly to loader.dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    patient_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, y) in enumerate(loader):\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs).squeeze(1)\n",
    "\n",
    "            batch_size = imgs.size(0)\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            preds.extend(out.cpu().numpy())\n",
    "            gts.extend(y.numpy())\n",
    "            patient_ids.extend(\n",
    "                df_subset.iloc[start:end][\"patient_id\"].values\n",
    "            )\n",
    "\n",
    "    return np.array(preds), np.array(gts), np.array(patient_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "662ca839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_level_metrics(preds, gts, patient_ids):\n",
    "    \"\"\"\n",
    "    preds: continuous predictions (float)\n",
    "    gts: true grades (1‚Äì4)\n",
    "    patient_ids: patient identifiers\n",
    "    \"\"\"\n",
    "\n",
    "    patient_preds = {}\n",
    "    patient_gts = {}\n",
    "\n",
    "    for p, y, pid in zip(preds, gts, patient_ids):\n",
    "        if pid not in patient_preds:\n",
    "            patient_preds[pid] = []\n",
    "            patient_gts[pid] = y\n",
    "        patient_preds[pid].append(p)\n",
    "\n",
    "    fused_preds = []\n",
    "    fused_gts = []\n",
    "\n",
    "    for pid in patient_preds:\n",
    "        fused_preds.append(np.mean(patient_preds[pid]))\n",
    "        fused_gts.append(patient_gts[pid])\n",
    "\n",
    "    fused_preds = np.array(fused_preds)\n",
    "    fused_gts = np.array(fused_gts)\n",
    "\n",
    "    # Metrics\n",
    "    mae = np.mean(np.abs(fused_preds - fused_gts))\n",
    "    rounded = np.clip(np.round(fused_preds), 1, 4)\n",
    "    off_by_1 = np.mean(np.abs(rounded - fused_gts) <= 1)\n",
    "\n",
    "    return mae, off_by_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a468d351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD_0 (PATIENT-LEVEL REGRESSION) =====\n",
      "fold_0 MAE: 0.936 | Off-by-1 Acc: 0.833\n",
      "\n",
      "===== FOLD_1 (PATIENT-LEVEL REGRESSION) =====\n",
      "fold_1 MAE: 0.650 | Off-by-1 Acc: 1.000\n",
      "\n",
      "===== FOLD_2 (PATIENT-LEVEL REGRESSION) =====\n",
      "fold_2 MAE: 0.777 | Off-by-1 Acc: 1.000\n",
      "\n",
      "===== FOLD_3 (PATIENT-LEVEL REGRESSION) =====\n",
      "fold_3 MAE: 0.768 | Off-by-1 Acc: 1.000\n",
      "\n",
      "===== FOLD_4 (PATIENT-LEVEL REGRESSION) =====\n",
      "fold_4 MAE: 0.919 | Off-by-1 Acc: 0.857\n"
     ]
    }
   ],
   "source": [
    "maes_fused = []\n",
    "off1s_fused = []\n",
    "\n",
    "for fold_name, split in folds.items():\n",
    "    print(f\"\\n===== {fold_name.upper()} (PATIENT-LEVEL REGRESSION) =====\")\n",
    "\n",
    "    train_df = df.iloc[split[\"train_idx\"]]\n",
    "    test_df  = df.iloc[split[\"test_idx\"]]\n",
    "\n",
    "    train_ds = ANDatasetReg(train_df, train_tfms)\n",
    "    test_ds  = ANDatasetReg(test_df, test_tfms)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(12):\n",
    "        train_one_epoch_reg(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "    # üî• patient-level evaluation\n",
    "    preds, gts, pids = eval_reg_patient_level(\n",
    "        model, test_loader, test_df, device\n",
    "    )\n",
    "\n",
    "    mae, off1 = patient_level_metrics(preds, gts, pids)\n",
    "    maes_fused.append(mae)\n",
    "    off1s_fused.append(off1)\n",
    "\n",
    "    print(f\"{fold_name} MAE: {mae:.3f} | Off-by-1 Acc: {off1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee49ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PATIENT-LEVEL REGRESSION CV RESULT =====\n",
      "MAE per fold: [np.float32(0.93570423), np.float32(0.65042776), np.float32(0.7766094), np.float32(0.7679024), np.float32(0.9185685)]\n",
      "Mean MAE: 0.810 ¬± 0.106\n",
      "Off-by-1 Acc per fold: [np.float64(0.8333333333333334), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(0.8571428571428571)]\n",
      "Mean Off-by-1 Acc: 0.938\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== PATIENT-LEVEL REGRESSION CV RESULT =====\")\n",
    "print(\"MAE per fold:\", maes_fused)\n",
    "print(f\"Mean MAE: {np.mean(maes_fused):.3f} ¬± {np.std(maes_fused):.3f}\")\n",
    "\n",
    "print(\"Off-by-1 Acc per fold:\", off1s_fused)\n",
    "print(f\"Mean Off-by-1 Acc: {np.mean(off1s_fused):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f4921",
   "metadata": {},
   "source": [
    "Treating Acanthosis Nigricans grading as an ordinal regression problem and aggregating predictions across multiple patient views substantially improves grading reliability and clinical relevance compared to standard image-level classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e4abc",
   "metadata": {},
   "source": [
    "# Quadratic Weighted Kappa(QWK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "931c99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efb97934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_level_metrics_with_qwk(preds, gts, patient_ids):\n",
    "    patient_preds = {}\n",
    "    patient_gts = {}\n",
    "\n",
    "    for p, y, pid in zip(preds, gts, patient_ids):\n",
    "        if pid not in patient_preds:\n",
    "            patient_preds[pid] = []\n",
    "            patient_gts[pid] = y\n",
    "        patient_preds[pid].append(p)\n",
    "\n",
    "    fused_preds = []\n",
    "    fused_gts = []\n",
    "\n",
    "    for pid in patient_preds:\n",
    "        fused_preds.append(np.mean(patient_preds[pid]))\n",
    "        fused_gts.append(patient_gts[pid])\n",
    "\n",
    "    fused_preds = np.array(fused_preds)\n",
    "    fused_gts = np.array(fused_gts)\n",
    "\n",
    "    rounded_preds = np.clip(np.round(fused_preds), 1, 4)\n",
    "\n",
    "    mae = np.mean(np.abs(fused_preds - fused_gts))\n",
    "    off_by_1 = np.mean(np.abs(rounded_preds - fused_gts) <= 1)\n",
    "\n",
    "    qwk = cohen_kappa_score(\n",
    "        fused_gts.astype(int),\n",
    "        rounded_preds.astype(int),\n",
    "        weights=\"quadratic\"\n",
    "    )\n",
    "\n",
    "    return mae, off_by_1, qwk, fused_preds, fused_gts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05586757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD_0 (PATIENT REG + QWK) =====\n",
      "MAE: 1.132 | Off-by-1: 0.667 | QWK: 0.108\n",
      "\n",
      "===== FOLD_1 (PATIENT REG + QWK) =====\n",
      "MAE: 0.624 | Off-by-1: 1.000 | QWK: 0.526\n",
      "\n",
      "===== FOLD_2 (PATIENT REG + QWK) =====\n",
      "MAE: 0.567 | Off-by-1: 1.000 | QWK: 0.640\n",
      "\n",
      "===== FOLD_3 (PATIENT REG + QWK) =====\n",
      "MAE: 0.713 | Off-by-1: 1.000 | QWK: 0.556\n",
      "\n",
      "===== FOLD_4 (PATIENT REG + QWK) =====\n",
      "MAE: 0.959 | Off-by-1: 0.714 | QWK: 0.189\n"
     ]
    }
   ],
   "source": [
    "maes, off1s, qwks = [], [], []\n",
    "\n",
    "all_preds = []\n",
    "all_gts = []\n",
    "\n",
    "for fold_name, split in folds.items():\n",
    "    print(f\"\\n===== {fold_name.upper()} (PATIENT REG + QWK) =====\")\n",
    "\n",
    "    train_df = df.iloc[split[\"train_idx\"]]\n",
    "    test_df  = df.iloc[split[\"test_idx\"]]\n",
    "\n",
    "    train_ds = ANDatasetReg(train_df, train_tfms)\n",
    "    test_ds  = ANDatasetReg(test_df, test_tfms)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    for epoch in range(12):\n",
    "        train_one_epoch_reg(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    preds, gts, pids = eval_reg_patient_level(\n",
    "        model, test_loader, test_df, device\n",
    "    )\n",
    "\n",
    "    mae, off1, qwk, fused_preds, fused_gts = patient_level_metrics_with_qwk(\n",
    "        preds, gts, pids\n",
    "    )\n",
    "\n",
    "    maes.append(mae)\n",
    "    off1s.append(off1)\n",
    "    qwks.append(qwk)\n",
    "\n",
    "    all_preds.extend(fused_preds)\n",
    "    all_gts.extend(fused_gts)\n",
    "\n",
    "    print(f\"MAE: {mae:.3f} | Off-by-1: {off1:.3f} | QWK: {qwk:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b87f2",
   "metadata": {},
   "source": [
    "# 4th Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c46cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50946a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>grade</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/cropped_images/AN_1_G4_back_0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/cropped_images/AN_1_G4_neck_1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/cropped_images/AN_1_G4_neck_2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/cropped_images/AN_1_G4_neck_3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/cropped_images/AN_2_G3_back_0.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_path  patient_id  grade  view\n",
       "0  ../data/cropped_images/AN_1_G4_back_0.jpg           1      4  back\n",
       "1  ../data/cropped_images/AN_1_G4_neck_1.jpg           1      4  neck\n",
       "2  ../data/cropped_images/AN_1_G4_neck_2.jpg           1      4  neck\n",
       "3  ../data/cropped_images/AN_1_G4_neck_3.jpg           1      4  neck\n",
       "4  ../data/cropped_images/AN_2_G3_back_0.jpg           2      3  back"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "df = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "with open(\"../splits/cv_folds.json\", \"r\") as f:\n",
    "    folds = json.load(f)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_lab_l(img_pil):\n",
    "    img = np.array(img_pil)\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    L = lab[:, :, 0]          # shape (H, W)\n",
    "    return Image.fromarray(L, mode=\"L\")\n",
    "\n",
    "\n",
    "class ANDatasetRegLAB(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        img = rgb_to_lab_l(img)   # üî• key change\n",
    "\n",
    "        y = float(row[\"grade\"])  # keep 1‚Äì4\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9743896",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f5f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_loss(logits, labels):\n",
    "    labels = labels.long() - 1  # 1‚Äì4 ‚Üí 0‚Äì3\n",
    "    targets = torch.zeros((labels.size(0), 3), device=labels.device)\n",
    "\n",
    "    for i in range(3):\n",
    "        targets[:, i] = (labels > i).float()\n",
    "\n",
    "    return nn.BCEWithLogitsLoss()(logits, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70ac062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_ordinal(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, y in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = ordinal_loss(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ff8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ordinal_patient_level(model, loader, df_subset):\n",
    "    model.eval()\n",
    "    preds, gts, pids = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, y) in enumerate(loader):\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            pred = torch.sum(probs > 0.5, dim=1) + 1\n",
    "\n",
    "            batch_size = imgs.size(0)\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            gts.extend(y.numpy())\n",
    "            pids.extend(df_subset.iloc[start:end][\"patient_id\"].values)\n",
    "\n",
    "    return np.array(preds), np.array(gts), np.array(pids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255a9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_level_metrics_with_qwk(preds, gts, patient_ids):\n",
    "    patient_preds = {}\n",
    "    patient_gts = {}\n",
    "\n",
    "    for p, y, pid in zip(preds, gts, patient_ids):\n",
    "        if pid not in patient_preds:\n",
    "            patient_preds[pid] = []\n",
    "            patient_gts[pid] = y\n",
    "        patient_preds[pid].append(p)\n",
    "\n",
    "    fused_preds = []\n",
    "    fused_gts = []\n",
    "\n",
    "    for pid in patient_preds:\n",
    "        fused_preds.append(np.mean(patient_preds[pid]))\n",
    "        fused_gts.append(patient_gts[pid])\n",
    "\n",
    "    fused_preds = np.array(fused_preds)\n",
    "    fused_gts = np.array(fused_gts)\n",
    "\n",
    "    mae = np.mean(np.abs(fused_preds - fused_gts))\n",
    "    off_by_1 = np.mean(np.abs(np.round(fused_preds) - fused_gts) <= 1)\n",
    "\n",
    "    qwk = cohen_kappa_score(\n",
    "        fused_gts.astype(int),\n",
    "        np.clip(np.round(fused_preds), 1, 4).astype(int),\n",
    "        weights=\"quadratic\"\n",
    "    )\n",
    "\n",
    "    return mae, off_by_1, qwk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94738eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD_0 (ORDINAL + LAB-L + PATIENT FUSION) =====\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 1), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pranathi\\Desktop\\Pranathi\\Projects\\Acanthosis_Nigricans\\venv\\Lib\\site-packages\\PIL\\Image.py:3304\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3304\u001b[39m     typemode, rawmode, color_modes = \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3305\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyError\u001b[39m: ((1, 1, 1), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m15\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     loss = \u001b[43mtrain_one_epoch_ordinal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m preds, gts, pids = eval_ordinal_patient_level(model, test_loader, test_df)\n\u001b[32m     26\u001b[39m mae, off1, qwk = patient_level_metrics_with_qwk(preds, gts, pids)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtrain_one_epoch_ordinal\u001b[39m\u001b[34m(model, loader, optimizer)\u001b[39m\n\u001b[32m      2\u001b[39m model.train()\n\u001b[32m      3\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pranathi\\Desktop\\Pranathi\\Projects\\Acanthosis_Nigricans\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pranathi\\Desktop\\Pranathi\\Projects\\Acanthosis_Nigricans\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pranathi\\Desktop\\Pranathi\\Projects\\Acanthosis_Nigricans\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pranathi\\Desktop\\Pranathi\\Projects\\Acanthosis_Nigricans\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mANDatasetRegLAB.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     17\u001b[39m row = \u001b[38;5;28mself\u001b[39m.df.iloc[idx]\n\u001b[32m     19\u001b[39m img = Image.open(row[\u001b[33m\"\u001b[39m\u001b[33mimage_path\u001b[39m\u001b[33m\"\u001b[39m]).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m img = \u001b[43mrgb_to_lab_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# üî• key change\u001b[39;00m\n\u001b[32m     22\u001b[39m y = \u001b[38;5;28mfloat\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mgrade\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# keep 1‚Äì4\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrgb_to_lab_l\u001b[39m\u001b[34m(img_pil)\u001b[39m\n\u001b[32m      4\u001b[39m L = lab[:, :, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Lightness channel\u001b[39;00m\n\u001b[32m      5\u001b[39m L = np.expand_dims(L, axis=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pranathi\\Desktop\\Pranathi\\Projects\\Acanthosis_Nigricans\\venv\\Lib\\site-packages\\PIL\\Image.py:3308\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3306\u001b[39m         typekey_shape, typestr = typekey\n\u001b[32m   3307\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   3309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode != typemode \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m color_modes:\n",
      "\u001b[31mTypeError\u001b[39m: Cannot handle this data type: (1, 1, 1), |u1"
     ]
    }
   ],
   "source": [
    "maes, off1s, qwks = [], [], []\n",
    "\n",
    "for fold_name, split in folds.items():\n",
    "    print(f\"\\n===== {fold_name.upper()} (ORDINAL + LAB-L + PATIENT FUSION) =====\")\n",
    "\n",
    "    train_df = df.iloc[split[\"train_idx\"]]\n",
    "    test_df  = df.iloc[split[\"test_idx\"]]\n",
    "\n",
    "    train_ds = ANDatasetRegLAB(train_df, train_tfms)\n",
    "    test_ds  = ANDatasetRegLAB(test_df, test_tfms)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "    model.classifier[1] = nn.Linear(1280, 3)  # ordinal head\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(15):\n",
    "        loss = train_one_epoch_ordinal(model, train_loader, optimizer)\n",
    "\n",
    "    preds, gts, pids = eval_ordinal_patient_level(model, test_loader, test_df)\n",
    "\n",
    "    mae, off1, qwk = patient_level_metrics_with_qwk(preds, gts, pids)\n",
    "\n",
    "    maes.append(mae)\n",
    "    off1s.append(off1)\n",
    "    qwks.append(qwk)\n",
    "\n",
    "    print(f\"MAE: {mae:.3f} | Off-by-1: {off1:.3f} | QWK: {qwk:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d48db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== FINAL PATIENT-LEVEL RESULTS =====\")\n",
    "print(f\"Mean MAE: {np.mean(maes):.3f} ¬± {np.std(maes):.3f}\")\n",
    "print(f\"Mean Off-by-1 Accuracy: {np.mean(off1s):.3f}\")\n",
    "print(f\"Mean QWK: {np.mean(qwks):.3f} ¬± {np.std(qwks):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
